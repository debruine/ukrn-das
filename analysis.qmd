---
title: "Data Availability Statement Analysis"
format: 
  html:
    toc: true
    df-print: paged
    embed-resources: true
---

```{r, include = FALSE}
# devtools::install_github("scienceverse/papercheck")
library(papercheck) 
library(tidyverse)
library(readxl)
```

## Data Cleaning

### Manual DAS Categories

```{r}
sub_sarah <- read_xlsx("subset1_Sarah_WD.xlsx", range = "I1:I601")

count(sub_sarah, MANUAL_DAS_CAT) |> select(n, MANUAL_DAS_CAT)
```

```{r}
sub_peace <- read_xlsx("subset_Peace_WD.xlsx", range = "H1:H601")

count(sub_peace, MANUAL_DAS_CAT) |> select(n, MANUAL_DAS_CAT)
```

### Clean Up

```{r}
# read in data

sub_sarah <- read_xlsx("subset1_Sarah_WD.xlsx") |>
  filter(!is.na(MANUAL_DAS)) |>
  select(-downloaded) |>
  select(UNI_ID, MANUAL_DAS:DASLINKS) |>
  mutate(MANUAL_DAS = recode(MANUAL_DAS, "Yes" = TRUE, "No" = FALSE),
         DAS_TEXT = replace_na(DAS_TEXT, "None provided"),
         DAS_LOC = replace_na(DAS_LOC, "None provided"),
         MANUAL_DAS_CAT = replace_na(MANUAL_DAS_CAT, "Not available"),
         DASLINKS = replace_na(DASLINKS, "No link provided"))

sub_peace <- read_xlsx("subset_Peace_WD.xlsx") |>
  filter(!is.na(MANUAL_DAS)) |>
  select(UNI_ID, MANUAL_DAS:DASLINKS)
```

### Duplicates

```{r}
# investigate duplicates
dup_peace <- semi_join(sub_peace, sub_sarah, by = "UNI_ID")
dup_sarah <- semi_join(sub_sarah, sub_peace, by = "UNI_ID")

dupes <- bind_rows(dup_peace, dup_sarah) |>
  arrange(UNI_ID)

unique(dupes)
```

### Join Data

```{r}
# use peace's duplicates
manual_data <- anti_join(sub_sarah, sub_peace, by = "UNI_ID") |>
  bind_rows(sub_peace)

paper_data <- read_csv("subset1.csv", show_col_types = FALSE) |>
  select(UNI_ID:PUB_DAS) |> 
  left_join(manual_data, by = "UNI_ID") |>
  mutate(UNI_DAS = recode(UNI_DAS, "Yes" = TRUE, "No" = FALSE))

```

## Consistency

### Pub/Uni vs manual coding

```{r}
count(paper_data, UNI_DAS, PUB_DAS, MANUAL_DAS) |>
  arrange(UNI_DAS, PUB_DAS) |>
  pivot_wider(names_from = MANUAL_DAS, values_from = n, 
              names_prefix = "MAN_") |>
  mutate(
    uni_agree = ifelse(UNI_DAS, `MAN_TRUE`, `MAN_FALSE`)/100,
    pub_agree = ifelse(PUB_DAS, `MAN_TRUE`, `MAN_FALSE`)/100
  )
```

### Uni vs manual coding

```{r}
count(paper_data, UNI_DAS, MANUAL_DAS) |>
  pivot_wider(names_from = MANUAL_DAS, values_from = n, 
              names_prefix = "MANUAL_")
```

### Pub vs manual coding

```{r}
count(paper_data, PUB_DAS, MANUAL_DAS) |>
  pivot_wider(names_from = MANUAL_DAS, values_from = n, 
              names_prefix = "MANUAL_")
```




```{r, include = FALSE}
papers <- readRDS("subset1_papers.RDS")

# put all text in one table
all_text <- search_text(papers) |>
  mutate(UNI_ID = sub("\\.xml", "", id) |> as.numeric())
```

## Labelled Data Availability Sections

```{r}
# headers add spaces randomly
data_avail_pattern <- "d ?a ?t ?a\\s+a ?v ?a ?i ?l ?"

das_papers <- filter(all_text, 
              grepl(data_avail_pattern, header, TRUE), 
              text != header) |>
  summarise(text = paste(text, collapse = " "), 
            .by = c(UNI_ID, section, header)) |>
  left_join(paper_data, by = "UNI_ID") |>
  mutate(request = grepl("request", text))

n_total <- nrow(paper_data)
n_papers_with_das <- nrow(das_papers)

count(das_papers, UNI_DAS, PUB_DAS, MANUAL_DAS) |>
  pivot_wider(names_from = UNI_DAS, values_from = n, values_fill = 0)
```


```{r}
# this doesn't seem very useful
data_avail <- search_text(all_text, "data .* avail") |>
  filter(header != text) |>
  search_text(return = "paragraph")
```


## Common data sources

```{r}
osf <- search_text(all_text, "osf\\.io") |>
  search_text("data")
osf_link <- search_text(osf, "osf\\.io/[/a-z0-9]+", return = "match")
osf$link <- osf_link$text
osf
```

## Data plus a link

```{r}
# links often have spaces after / or .
#link_pattern <- "https?:// ?[a-z0-9_]*([\\./]\\s?[?a-z0-9_-]+)*[a-z0-9_-]"
link_pattern <- "https?://[ a-z0-9_]([\\./]?\\s?[?a-z0-9_=-]+)*[a-z0-9_-]"

data_http <- search_text(all_text, "data") |>
  search_text(link_pattern)

link <- search_text(data_http, link_pattern, return = "match") |>
  select(UNI_ID, link = text, div, p, s) |>
  mutate(link = gsub("\\s", "", link))
data_http <- left_join(data_http, link, by = c("UNI_ID", "div", "p", "s")) |>
  left_join(paper_data[, c('UNI_ID', 'DASLINKS')], by = "UNI_ID")

data_http |> select(UNI_ID, link, DASLINKS, text)
```



